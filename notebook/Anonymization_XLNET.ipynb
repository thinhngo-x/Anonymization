{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python (INF442)",
      "language": "python",
      "name": "inf442_pi9"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Anonymization_XLNET.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thinhngo-x/Anonymization/blob/master/notebook/Anonymization_XLNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fTFt6cf6ITT",
        "colab_type": "text"
      },
      "source": [
        "### Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvAmCYtGoKk-",
        "colab_type": "code",
        "outputId": "0a2e2440-ec66-4c22-c921-6abd5975b5d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root = '/content/drive/My Drive/Anonymization/CONLL2003_XLNET/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw7DBPEgdcoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install transformers --quiet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vPayx1sda4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transformers\n",
        "import os\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11A6KPFoda4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget -nc \"https://raw.githubusercontent.com/glample/tagger/master/dataset/eng.testa\" --quiet\n",
        "! wget -nc \"https://raw.githubusercontent.com/glample/tagger/master/dataset/eng.train\" --quiet\n",
        "! wget -nc \"https://raw.githubusercontent.com/glample/tagger/master/dataset/eng.testb\" --quiet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWsk32s7da40",
        "colab_type": "text"
      },
      "source": [
        "### Some useful functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc1cSBdqda41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for token classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, words, labels):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            words: list. The words of the sequence.\n",
        "            labels: (Optional) list. The labels for each word of the sequence. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.words = words\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_ids = label_ids\n",
        "\n",
        "\n",
        "def read_examples_from_file(data_dir, mode=\"eng.testa\"):\n",
        "    file_path = os.path.join(data_dir, \"{}\".format(mode))\n",
        "    guid_index = 1\n",
        "    examples = []\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        words = []\n",
        "        labels = []\n",
        "        for line in f:\n",
        "            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
        "                if words:\n",
        "                    examples.append(InputExample(guid=\"{}-{}\".format(mode, guid_index), words=words, labels=labels))\n",
        "                    guid_index += 1\n",
        "                    words = []\n",
        "                    labels = []\n",
        "            else:\n",
        "                splits = line.split(\" \")\n",
        "                words.append(splits[0])\n",
        "                if len(splits) > 1:\n",
        "                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n",
        "                else:\n",
        "                    # Examples could have no label for mode = \"test\"\n",
        "                    labels.append(\"O\")\n",
        "        if words:\n",
        "            examples.append(InputExample(guid=\"{}-{}\".format(mode, guid_index), words=words, labels=labels))\n",
        "    return examples\n",
        "\n",
        "\n",
        "def convert_examples_to_features(\n",
        "    examples,\n",
        "    label_list,\n",
        "    max_seq_length,\n",
        "    tokenizer,\n",
        "    cls_token_at_end=False,\n",
        "    cls_token=\"[CLS]\",\n",
        "    cls_token_segment_id=1,\n",
        "    sep_token=\"[SEP]\",\n",
        "    sep_token_extra=False,\n",
        "    pad_on_left=False,\n",
        "    pad_token=0,\n",
        "    pad_token_segment_id=0,\n",
        "    pad_token_label_id=-100,\n",
        "    sequence_a_segment_id=0,\n",
        "    mask_padding_with_zero=True,\n",
        "):\n",
        "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
        "        `cls_token_at_end` define the location of the CLS token:\n",
        "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
        "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
        "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
        "    \"\"\"\n",
        "\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 10000 == 0:\n",
        "            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n",
        "\n",
        "        tokens = []\n",
        "        label_ids = []\n",
        "        for word, label in zip(example.words, example.labels):\n",
        "            word_tokens = tokenizer.tokenize(word)\n",
        "            tokens.extend(word_tokens)\n",
        "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
        "            label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
        "\n",
        "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
        "        special_tokens_count = 3 if sep_token_extra else 2\n",
        "        if len(tokens) > max_seq_length - special_tokens_count:\n",
        "            tokens = tokens[: (max_seq_length - special_tokens_count)]\n",
        "            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids:   0   0   0   0  0     0   0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens += [sep_token]\n",
        "        label_ids += [pad_token_label_id]\n",
        "        if sep_token_extra:\n",
        "            # roberta uses an extra separator b/w pairs of sentences\n",
        "            tokens += [sep_token]\n",
        "            label_ids += [pad_token_label_id]\n",
        "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "        if cls_token_at_end:\n",
        "            tokens += [cls_token]\n",
        "            label_ids += [pad_token_label_id]\n",
        "            segment_ids += [cls_token_segment_id]\n",
        "        else:\n",
        "            tokens = [cls_token] + tokens\n",
        "            label_ids = [pad_token_label_id] + label_ids\n",
        "            segment_ids = [cls_token_segment_id] + segment_ids\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding_length = max_seq_length - len(input_ids)\n",
        "        if pad_on_left:\n",
        "            input_ids = ([pad_token] * padding_length) + input_ids\n",
        "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
        "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
        "            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n",
        "        else:\n",
        "            input_ids += [pad_token] * padding_length\n",
        "            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
        "            segment_ids += [pad_token_segment_id] * padding_length\n",
        "            label_ids += [pad_token_label_id] * padding_length\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "        assert len(label_ids) == max_seq_length\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\", example.guid)\n",
        "            logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids]))\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_ids=label_ids)\n",
        "        )\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QfRrCqEda4_",
        "colab_type": "text"
      },
      "source": [
        "### Obtaining the representation of the dataset from XLNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nwMVKuGda4_",
        "colab_type": "text"
      },
      "source": [
        "#### An example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3h3Obe4da5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import XLNetTokenizer, XLNetModel\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FavsZF4yda5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = XLNetModel.from_pretrained(\"xlnet-large-cased\").cuda()\n",
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-large-cased\")\n",
        "\n",
        "sequence = \"Adrien Ehrhardt donne un projet d'informatique aux étudiants de INF442.\"\n",
        "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
        "inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model(inputs.cuda())[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtSKdTMeda5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs.detach().cpu().numpy()[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ds8scvkda5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0yH7hsWzM-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHhzG_pWda5L",
        "colab_type": "text"
      },
      "source": [
        "#### The real thing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od57D9FOda5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrG1tTdUda5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm, trange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn9xIXY5da5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuNLGm-Fda5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_token_label_id=CrossEntropyLoss().ignore_index\n",
        "labels = [\"O\", \"B-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n",
        "model = XLNetModel.from_pretrained(\"xlnet-large-cased\").cuda()\n",
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-large-cased\")\n",
        "examples = read_examples_from_file(\".\", mode=\"eng.testa\")\n",
        "features = convert_examples_to_features(\n",
        "            examples,\n",
        "            label_list = labels,\n",
        "            max_seq_length = 128,\n",
        "            tokenizer = tokenizer,\n",
        "            cls_token_at_end=True,\n",
        "            cls_token=tokenizer.cls_token,\n",
        "            cls_token_segment_id=2,\n",
        "            sep_token=tokenizer.sep_token,\n",
        "            sep_token_extra=False,\n",
        "            pad_on_left=False,\n",
        "            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
        "            pad_token_segment_id=0,\n",
        "            pad_token_label_id=pad_token_label_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPz0UJrida5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to Tensors and build dataset\n",
        "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
        "\n",
        "dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnCUlAfNda5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_sampler = SequentialSampler(dataset)\n",
        "eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=32)\n",
        "preds = None\n",
        "out_label_ids = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAoyAuOTda5a",
        "colab_type": "code",
        "outputId": "2ed51e3a-d89d-4d43-b38c-f64d9d6b2910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.eval()\n",
        "preds = []\n",
        "out_label_ids = []\n",
        "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "    batch = tuple(t.to(\"cuda\") for t in batch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1]}\n",
        "        inputs[\"token_type_ids\"] = (\n",
        "            batch[2]\n",
        "        )\n",
        "        outputs = model(**inputs)\n",
        "        last_hidden_layer = outputs[0]\n",
        "        preds.append(last_hidden_layer.detach().cpu().numpy())\n",
        "        out_label_ids.append(batch[3].detach().cpu().numpy())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 102/102 [02:00<00:00,  1.18s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRNbBdKrda5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out_label_list = [[] for _ in range(len(out_label_ids) * 32)]\n",
        "preds_list = [[] for _ in range(len(out_label_ids) * 32)]\n",
        "\n",
        "label_map = {i: label for i, label in enumerate(labels)}\n",
        "        \n",
        "for i in range(len(out_label_ids)):\n",
        "    for b in range(out_label_ids[i].shape[0]):\n",
        "        for j in range(out_label_ids[0].shape[1]):\n",
        "            if out_label_ids[i][b,j] != pad_token_label_id:\n",
        "                out_label_list[i*32+b].append(out_label_ids[i][b,j])\n",
        "                preds_list[i*32+b].append(preds[i][b,j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sQxpSYsda5f",
        "colab_type": "text"
      },
      "source": [
        "`out_label_list` is of shape (#number of sentences, #tokens in this sentence, 1)\n",
        "\n",
        "`preds_list` is of shape (#number of sentences, #tokens in this sentence, 1024 hidden representations)\n",
        "\n",
        "For the sake of simplicity, we'll consider that an observation / a sample is a token, not a sentence (the contextual meaning of each token is already taken into account in its representation), so we need to \"flatten\" both lists so that they're of shape (#tokens, 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mXeyeFmda5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wke-7lB0da5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_labels = np.array(list(itertools.chain.from_iterable(out_label_list)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJKEZxESda5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flat_list = []\n",
        "for sentence in preds_list:\n",
        "    for token in sentence:\n",
        "        flat_list.append(token)\n",
        "        \n",
        "representation = np.array(flat_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te_PwQvVda5m",
        "colab_type": "code",
        "outputId": "1ea02ff1-ac82-435f-f51c-0123cac11afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "true_labels.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(51236,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBdJ7I-Bda5o",
        "colab_type": "code",
        "outputId": "087699b3-ddf7-4d04-9f61-9fefe9de7686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "representation.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(51236, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rWuf_bAda5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(root+\"true_labels.train.npy\", true_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2wylJH9da5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(root+\"representation.train.npy\", representation)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}